{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02HPE-DL_Project",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sonalgan/DeepLearning/blob/main/02HPE_DL_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaxaOsC5sUci"
      },
      "source": [
        "Downloading Dataset\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4xuTb22V3SK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96d3ee35-eadd-4043-f4ee-5c1e28e656b9"
      },
      "source": [
        "!wget -q http://lotus.kuee.kyoto-u.ac.jp/WAT/indic-multilingual/indic_wat_2021.tar.gz\r\n",
        "!tar xzf indic_wat_2021.tar.gz\r\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git -q\r\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git -q\r\n",
        "!mkdir data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'indic_nlp_resources' already exists and is not an empty directory.\n",
            "fatal: destination path 'indic_nlp_library' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4yhowko75Ob"
      },
      "source": [
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoMtlMtdoXCh"
      },
      "source": [
        "punctuation=[\",\",\"।\",\";\",\"?\",\"!\",\"—\",\"‐\",\":-\",\"'\",\"(\",\")\",'\"']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d87PN_2dXtry"
      },
      "source": [
        "languages=[\"hi\",\"bn\",\"gu\",\"kn\",\"mr\",\"or\",\"ta\",\"te\",\"ml\",\"pa\"]\r\n",
        "languages.sort()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0MP6OTNGwUp"
      },
      "source": [
        "import sys\r\n",
        "# The path to the local git repo for Indic NLP library\r\n",
        "INDIC_NLP_LIB_HOME=r\"/content/indic_nlp_library\"\r\n",
        "# The path to the local git repo for Indic NLP Resources\r\n",
        "INDIC_NLP_RESOURCES=r\"/content/indic_nlp_resources\"\r\n",
        "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\r\n",
        "from indicnlp import common\r\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)\r\n",
        "from indicnlp import loader\r\n",
        "loader.load()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1VkIP3luPj1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4daf0a6-b06e-49e1-985e-3aa0c4039065"
      },
      "source": [
        "import string\r\n",
        "for char in string.punctuation:\r\n",
        "  print(char,end=\" \")\r\n",
        "translate_table=dict((ord(char),None) for char in string.punctuation) \r\n",
        "global translate_table"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "! \" # $ % & ' ( ) * + , - . / : ; < = > ? @ [ \\ ] ^ _ ` { | } ~ "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQeemrPYqHYw"
      },
      "source": [
        "from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\r\n",
        "from indicnlp.tokenize import sentence_tokenize \r\n",
        "from re import sub as resub\r\n",
        "import csv\r\n",
        "\r\n",
        "factory=IndicNormalizerFactory()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XACo9tISOhDP"
      },
      "source": [
        "Pipeline for Tokenizing the sentences while removing special char, or roman script\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDpiYve7axzV",
        "outputId": "33cd98e8-fac0-43ce-8475-00907018c2fe"
      },
      "source": [
        "\r\n",
        "class LangCleaner:\r\n",
        "  def __init__(self):\r\n",
        "    self.lang=\"hi\"\r\n",
        "    self.normalizer=None\r\n",
        "\r\n",
        "  def loader(self,lang):\r\n",
        "    self.lang=lang\r\n",
        "    self.fields=[\"sentences\",\"length\"] \r\n",
        "    \r\n",
        "    fp = \"finalrepo/train/pmi/en-{}/train.{}\".format(lang,lang)\r\n",
        "    with open(fp) as f:\r\n",
        "      chunk = f.readlines()\r\n",
        "      self.normalizer=factory.get_normalizer(lang)\r\n",
        "    return chunk\r\n",
        "  def cleaner(self,chunk):\r\n",
        "    vocab=[]\r\n",
        "    len_sentence=[]\r\n",
        "    for line in chunk:\r\n",
        "      line=line.rstrip(\"\\n\")\r\n",
        "      line=self.normalizer.normalize(line)\r\n",
        "      line= resub(r\"\\d+\",\"\",line)\r\n",
        "      line=resub(r\"[a-zA-Z]+\",\"\",line)\r\n",
        "      # line=resub(r'[\\u0964\\u2022\\u00B7]','',line)\r\n",
        "      line=line.translate(translate_table)\r\n",
        "      sentences=sentence_tokenize.sentence_split(line, lang=self.lang)\r\n",
        "      for sentence in sentences:\r\n",
        "        vocab.append(sentence)\r\n",
        "        len_sentence.append(len(sentence))\r\n",
        "    with open('data/{}.csv'.format(self.lang), 'w',encoding=\"utf-8\") as outputfile:\r\n",
        "      csvwriter=csv.writer(outputfile)\r\n",
        "      csvwriter.writerow(self.fields)\r\n",
        "      for i, (x, y) in enumerate(zip(vocab, len_sentence)):\r\n",
        "        csvwriter.writerow([x,y])\r\n",
        "    return len(vocab)\r\n",
        "obj=LangCleaner()\r\n",
        "for lang in languages:\r\n",
        "  ans=obj.loader(lang)\r\n",
        "  len_data=obj.cleaner(ans)\r\n",
        "  print(\"Length of\",lang,\"dataset:\",len_data)\r\n",
        "del obj\r\n",
        "del factory\r\n",
        "del loader\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of bn dataset: 23657\n",
            "Length of gu dataset: 41578\n",
            "Length of hi dataset: 50412\n",
            "Length of kn dataset: 28902\n",
            "Length of ml dataset: 26916\n",
            "Length of mr dataset: 28976\n",
            "Length of or dataset: 32017\n",
            "Length of pa dataset: 28414\n",
            "Length of ta dataset: 32638\n",
            "Length of te dataset: 33380\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kwLBqDspYeC"
      },
      "source": [
        "'''\r\n",
        "import os\r\n",
        "filelist = [ f for f in os.listdir(\"data\") ]\r\n",
        "for f in filelist:\r\n",
        "    os.remove(os.path.join(\"data\",f))\r\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}