{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02HPE-DL_Project",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sonalgan/DeepLearning/blob/master/02.1HPE_DL_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaxaOsC5sUci"
      },
      "source": [
        "Downloading Dataset\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4xuTb22V3SK"
      },
      "source": [
        "!wget -q http://lotus.kuee.kyoto-u.ac.jp/WAT/indic-multilingual/indic_wat_2021.tar.gz\r\n",
        "!tar xzf indic_wat_2021.tar.gz\r\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git -q\r\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git -q\r\n",
        "!mkdir data"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4yhowko75Ob"
      },
      "source": [
        "\r\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoMtlMtdoXCh"
      },
      "source": [
        "punctuation=[\",\",\"।\",\";\",\"?\",\"!\",\"—\",\"‐\",\":-\",\"'\",\"(\",\")\",'\"']"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d87PN_2dXtry"
      },
      "source": [
        "languages=[\"hi\",\"bn\",\"gu\",\"kn\",\"mr\",\"or\",\"ta\",\"te\",\"ml\",\"pa\"]\r\n",
        "languages.sort()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq2DktYbTeoZ"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0MP6OTNGwUp"
      },
      "source": [
        "import sys\r\n",
        "# The path to the local git repo for Indic NLP library\r\n",
        "INDIC_NLP_LIB_HOME=r\"/content/indic_nlp_library\"\r\n",
        "# The path to the local git repo for Indic NLP Resources\r\n",
        "INDIC_NLP_RESOURCES=r\"/content/indic_nlp_resources\"\r\n",
        "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\r\n",
        "from indicnlp import common\r\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)\r\n",
        "from indicnlp import loader\r\n",
        "loader.load()\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1VkIP3luPj1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eec37f5-2e9f-4a25-d151-13be51996159"
      },
      "source": [
        "import string\r\n",
        "for char in string.punctuation:\r\n",
        "  print(char,end=\" \")\r\n",
        "translate_table=dict((ord(char),None) for char in string.punctuation) \r\n",
        "global translate_table"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "! \" # $ % & ' ( ) * + , - . / : ; < = > ? @ [ \\ ] ^ _ ` { | } ~ "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ-a-GlkuJhv"
      },
      "source": [
        "from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\r\n",
        "factory=IndicNormalizerFactory()\r\n",
        "from indicnlp.tokenize import sentence_tokenize"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1mh4nyatPTv"
      },
      "source": [
        "import csv\r\n",
        "fields=[\"sentences\",\"lang\",\"length\"]\r\n",
        "with open('data/sentences.csv', 'w',encoding=\"utf-8\") as outputfile:\r\n",
        "   csvwriter=csv.writer(outputfile)\r\n",
        "   csvwriter.writerow(fields)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XACo9tISOhDP"
      },
      "source": [
        "Pipeline for Tokenizing the sentences while removing special char, or roman script\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDpiYve7axzV",
        "outputId": "d4a1303e-ccb9-4edb-d4f3-b3a8f04dd8d6"
      },
      "source": [
        "from statistics import mean,median,mode \r\n",
        "import os\r\n",
        "from re import sub as resub\r\n",
        "class LangCleaner:\r\n",
        "  def __init__(self):\r\n",
        "    self.lang=\"hi\"\r\n",
        "    self.normalizer=None\r\n",
        "\r\n",
        "  def loader(self,lang,fp):\r\n",
        "    self.lang=lang\r\n",
        "    \r\n",
        "    #fp = \"finalrepo/train/pmi/en-{}/train.{}\".format(lang,lang)\r\n",
        "    with open(fp) as f:\r\n",
        "      chunk = f.readlines()\r\n",
        "    self.normalizer=factory.get_normalizer(lang)\r\n",
        "    return chunk\r\n",
        "  def cleaner(self,chunk):\r\n",
        "    vocab=[]\r\n",
        "    len_sentence=[]\r\n",
        "    for line in chunk:\r\n",
        "      line=line.rstrip(\"\\n\")\r\n",
        "      line=self.normalizer.normalize(line)\r\n",
        "      line= resub(r\"\\d+\",\"\",line)\r\n",
        "      line=resub(r\"[a-zA-Z]+\",\"\",line)\r\n",
        "      # line=resub(r'[\\u0964\\u2022\\u00B7]','',line)\r\n",
        "      line=line.translate(translate_table)\r\n",
        "      sentences=sentence_tokenize.sentence_split(line, lang=self.lang)\r\n",
        "      \r\n",
        "\r\n",
        "      for sentence in sentences:\r\n",
        "        if(len(sentence)>200):\r\n",
        "          temp=sentence[200:]\r\n",
        "          sentence=sentence[:200]\r\n",
        "          vocab.append(temp)\r\n",
        "          len_sentence.append(len(temp))\r\n",
        "        vocab.append(sentence)\r\n",
        "        len_sentence.append(len(sentence))\r\n",
        "    with open('data/sentences.csv', 'a',encoding=\"utf-8\") as outputfile:\r\n",
        "      csvwriter=csv.writer(outputfile)\r\n",
        "      ##csvwriter.writerow(self.fields)\r\n",
        "      for i, (x, y) in enumerate(zip(vocab, len_sentence)):\r\n",
        "        csvwriter.writerow([x,self.lang,y])\r\n",
        "    return len(vocab)\r\n",
        "obj=LangCleaner()\r\n",
        "m=[]\r\n",
        "for lang in languages:\r\n",
        "  for dir,subdir,files in os.walk(\"finalrepo/train\"):\r\n",
        "    for file in files:\r\n",
        "      if(lang not in file):\r\n",
        "        continue\r\n",
        "      fp=os.path.join(dir,file)\r\n",
        "      ans=obj.loader(lang,fp)\r\n",
        "      len_data=obj.cleaner(ans)\r\n",
        "  print(\"Length of\",lang,\"dataset:\",len_data)\r\n",
        "  m.append(len_data)\r\n",
        "print(\"Mean: \",mean(m))\r\n",
        "print(\"Median : \",median(m))\r\n",
        "\r\n",
        "del obj\r\n",
        "del factory\r\n",
        "del loader\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of bn dataset: 12691\n",
            "Length of gu dataset: 16355\n",
            "Length of hi dataset: 51523\n",
            "Length of kn dataset: 2375\n",
            "Length of ml dataset: 6479\n",
            "Length of mr dataset: 23411\n",
            "Length of or dataset: 36373\n",
            "Length of pa dataset: 831\n",
            "Length of ta dataset: 12039\n",
            "Length of te dataset: 5501\n",
            "Mean:  16757.8\n",
            "Median :  12365.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm9zdK8diS1O"
      },
      "source": [
        "alphabets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjOTqZoHhWal"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kwLBqDspYeC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "8e3d1be7-9f46-4cf7-9a93-2207bc4d9a8e"
      },
      "source": [
        "\r\n",
        "filelist = [ f for f in os.listdir(\"data\") ]\r\n",
        "for f in filelist:\r\n",
        "    os.remove(os.path.join(\"data\",f))\r\n",
        "    \r\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IsADirectoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-f4e09fa50c29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfilelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilelist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: 'data/.ipynb_checkpoints'"
          ]
        }
      ]
    }
  ]
}